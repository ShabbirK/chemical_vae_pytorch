{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 120\n",
    "NLATENT = 196\n",
    "DECODER_HIDDEN_SIZE = 488\n",
    "DECODER_NUM_LAYERS = 3\n",
    "\n",
    "DATA_DIR = './data'\n",
    "DATA_FILE_NAME = '250k_rndm_zinc_drugs_clean_3.csv'\n",
    "CHAR_FILE_NAME = 'zinc.json'\n",
    "TEST_IDX_FILE_NAME = 'test_idx.npy'\n",
    "\n",
    "MODELS_DIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEUtils:\n",
    "    \n",
    "    def __init__(self, data_dir=DATA_DIR, \n",
    "                 data_file_name=DATA_FILE_NAME, \n",
    "                 char_file_name=CHAR_FILE_NAME, \n",
    "                 test_idx_file_name=TEST_IDX_FILE_NAME,\n",
    "                 max_len=MAX_LEN):\n",
    "        \n",
    "        self.data_dir = pathlib.Path(data_dir)\n",
    "        self.data_file = self.data_dir / pathlib.Path(data_file_name)\n",
    "        self.char_file = self.data_dir / pathlib.Path(char_file_name)\n",
    "        self.test_file = self.data_dir / pathlib.Path(test_idx_file_name)\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        all_letters = yaml.safe_load(open(self.char_file)) + ['SOS']\n",
    "        self.all_letters = all_letters\n",
    "        self.n_letters = len(all_letters)\n",
    "        self.letters_to_indices_dict = dict((l, i) for i, l in enumerate(all_letters))\n",
    "        self.indices_to_letters_dict = dict((i, l) for i, l in enumerate(all_letters))\n",
    "               \n",
    "    def get_data_df(self):\n",
    "        df = pd.read_csv(self.data_file)\n",
    "        df = df[df.smiles.str.len() <= self.max_len].reset_index(drop=True)\n",
    "        \n",
    "        # preprocess input smile to remove the newline character and add padding\n",
    "        df.loc[:, 'smiles'] = df.loc[:, 'smiles'].str.strip()\\\n",
    "                    .str.pad(width=self.max_len, side='right', fillchar=\" \")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    # One-hot matrix of first to last letters (not including EOS) for input\n",
    "    def get_input_tensor(self, smile):\n",
    "        tensor = torch.zeros(1, len(smile), self.n_letters) # batch_size * seq_length * num_features\n",
    "        for i, letter in enumerate(smile):\n",
    "            tensor[0][i][self.letters_to_indices_dict[letter]] = 1\n",
    "        return tensor\n",
    "\n",
    "    # LongTensor of first letter to end (EOS) for target\n",
    "    def get_target_tensor(self, smile):\n",
    "        letter_indexes = [self.letters_to_indices_dict[l] for l in smile]\n",
    "        # letter_indexes.append(self.n_letters - 1) # EOS\n",
    "        return torch.LongTensor(letter_indexes)\n",
    "    \n",
    "    def get_train_valid_test_splits(self, reg_col, valid_pct=.1):\n",
    "        df = self.get_data_df()[['smiles', reg_col]]\n",
    "        df = df.rename(columns={reg_col: 'reg_col'})\n",
    "        \n",
    "        test_idx = np.load(self.test_file)\n",
    "        non_test_idx = np.array(df[~df.index.isin(test_idx)].index)\n",
    "        train_idx, valid_idx = train_test_split(non_test_idx, test_size=valid_pct, \n",
    "                                                random_state=42, shuffle=True)\n",
    "        \n",
    "        assert len(df) == len(test_idx) + len(train_idx) + len(valid_idx)\n",
    "        \n",
    "        return df, train_idx, valid_idx, test_idx\n",
    "         \n",
    "    def get_dl(self, df, idx, bs, device, shuffle=False):\n",
    "        \n",
    "        df = df.iloc[idx]\n",
    "        \n",
    "        input_tensors = torch.zeros(len(df), self.max_len, self.n_letters)\n",
    "        target_tensors = torch.zeros(len(df), self.max_len, dtype=torch.long)\n",
    "        for i, smile in enumerate(df.smiles):\n",
    "            input_tensors[i] = self.get_input_tensor(smile)\n",
    "            target_tensors[i] = self.get_target_tensor(smile)\n",
    "        \n",
    "        input_tensors = input_tensors.to(device)\n",
    "        target_tensors = target_tensors.to(device)\n",
    "        \n",
    "        original_lengths = torch.tensor(df.smiles.str.strip().str.len().to_numpy()).to(device)\n",
    "\n",
    "        property_values = torch.tensor(df.reg_col.to_numpy()).type(torch.float32).to(device)\n",
    "         \n",
    "        ds = TensorDataset(input_tensors, target_tensors, original_lengths, property_values)\n",
    "        dl = DataLoader(ds, shuffle=shuffle, batch_size=bs)\n",
    "        \n",
    "        return dl\n",
    "    \n",
    "vae_utils = VAEUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_letters, nlatent, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.nlatent = nlatent\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            Lambda(lambda x: x.permute(0, 2, 1)), # the features are in the channels dimension\n",
    "            nn.Conv1d(in_channels=n_letters, out_channels=9, kernel_size=9),\n",
    "            nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.BatchNorm1d(9), # the authors of the paper did batch normalization\n",
    "            nn.Conv1d(in_channels=9, out_channels=9, kernel_size=9),\n",
    "            nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.BatchNorm1d(9), # the authors of the paper did batch normalization\n",
    "            nn.Conv1d(in_channels=9, out_channels=11, kernel_size=10),\n",
    "            nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.BatchNorm1d(11), # the authors of the paper did batch normalization\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.mean = nn.Linear(1045, nlatent)\n",
    "        self.log_var = nn.Linear(1045, nlatent)\n",
    "        self.dec_init_hidden = nn.Linear(nlatent, decoder_hidden_size)\n",
    "        \n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var) \n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mean + (eps * std) \n",
    "        return sample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mean(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        dec_init_hidden = self.dec_init_hidden(z)\n",
    "        \n",
    "        return z, mean, log_var, dec_init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_decoder_input(input_tensors):\n",
    "    '''\n",
    "    Adjust for SOS and make batch the second demension\n",
    "    '''\n",
    "    sos_tensor = torch.zeros(1, vae_utils.n_letters)\n",
    "    sos_tensor[0][vae_utils.letters_to_indices_dict['SOS']] = 1\n",
    "    new_tensor = torch.zeros(input_tensors.shape[0], vae_utils.max_len, vae_utils.n_letters)\n",
    "    new_tensor[:][0] = sos_tensor\n",
    "    new_tensor[:, 1:vae_utils.max_len, :] = input_tensors[:, 0:vae_utils.max_len-1, :]\n",
    "    new_tensor = new_tensor.permute(1, 0, 2).to(input_tensors.device)\n",
    "    \n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.prenum_layers = num_layers\n",
    "        self.preprocess = Lambda(preprocess_decoder_input)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.rnn(self.preprocess(input), )\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyPredictor(nn.Module):\n",
    "    def __init__(self, nlatent):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(nlatent, 1000),\n",
    "            nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(1000, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.predictor(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_anneal_function(epoch, anneal_start, k=1):\n",
    "    return 1 / (1 + np.exp(- k * (epoch - anneal_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    now = time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{m}m {s:.0f}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, anneal_start, lr, train_dl, valid_dl, device, reg_col:str):\n",
    "    encoder = Encoder(vae_utils.n_letters, NLATENT, DECODER_HIDDEN_SIZE).to(device)\n",
    "    decoder = Decoder(vae_utils.n_letters, DECODER_HIDDEN_SIZE, DECODER_NUM_LAYERS).to(device)\n",
    "    property_predictor = PropertyPredictor(NLATENT).to(device)\n",
    "    \n",
    "    enc_opt = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    dec_opt = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    pp_opt = optim.Adam(property_predictor.parameters(), lr=lr)\n",
    "    \n",
    "    recontruction_loss_func = nn.NLLLoss()\n",
    "    reg_loss_func = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    def process_dl(epoch, dl, train:bool):\n",
    "        num_loaders = len(dl)\n",
    "        loader_loss = 0\n",
    "        for input_tensors, target_tensors, original_lengths, property_values in dl:\n",
    "            z, mean, log_var, dec_init_hidden = encoder(input_tensors)\n",
    "            output, hidden = decoder(input_tensors, \n",
    "                                     dec_init_hidden.unsqueeze(0).repeat(DECODER_NUM_LAYERS, 1, 1))\n",
    "            reg_pred = property_predictor(z)\n",
    "            \n",
    "            kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "            kl_weight = kl_anneal_function(epoch, anneal_start)\n",
    "            \n",
    "            reg_loss = reg_loss_func(reg_pred.flatten(), property_values)\n",
    "            reg_loss = reg_loss.type(torch.float32) \n",
    "            \n",
    "            reconstruction_loss = 0\n",
    "            for i in range(input_tensors.shape[0]):\n",
    "                reconstruction_loss += recontruction_loss_func(output[i], target_tensors[i])\n",
    "            reconstruction_loss /= input_tensors.shape[0]\n",
    "            reconstruction_loss = reconstruction_loss.type(torch.float32) \n",
    "\n",
    "            loss = reconstruction_loss + kl_divergence * kl_weight + reg_loss\n",
    "            \n",
    "            if train:\n",
    "                enc_opt.zero_grad()\n",
    "                dec_opt.zero_grad()\n",
    "                pp_opt.zero_grad()\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                enc_opt.step()\n",
    "                dec_opt.step()\n",
    "                pp_opt.step()\n",
    "                \n",
    "            loader_loss += loss.item()\n",
    "        \n",
    "        return loader_loss / num_loaders\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        property_predictor.train()\n",
    "        train_loss = process_dl(epoch, train_dl, train=True)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        property_predictor.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = process_dl(epoch, valid_dl, train=False)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "        print(f\"Epoch: {epoch:3d} | Train Loss: {train_loss:10.5f} | Val Loss: {val_loss:10.5f} | \" +\\\n",
    "              f\"Time Taken: {time_since(start_time)}\")\n",
    "    \n",
    "    models_dir = pathlib.Path(MODELS_DIR)\n",
    "    models_dir.mkdir(exist_ok=True, parents=True)\n",
    "    torch.save(encoder.state_dict(), models_dir / pathlib.Path(f'{reg_col}_encoder.pth'))\n",
    "    torch.save(decoder.state_dict(), models_dir / pathlib.Path(f'{reg_col}_decoder.pth'))\n",
    "    torch.save(property_predictor.state_dict(), models_dir / pathlib.Path(f'{reg_col}_property_predictor.pth'))\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label = 'Validation Loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df, train_idx, valid_idx, test_idx):\n",
    "    start_time = time()\n",
    "    train_dl = vae_utils.get_dl(df, train_idx[:max_samples], bs, device, shuffle=True)\n",
    "    valid_dl = vae_utils.get_dl(df, valid_idx[:max_samples], bs*2, device)\n",
    "    test_dl = vae_utils.get_dl(df, test_idx[:max_samples], bs*2, device)\n",
    "    print(f'Time taken to get dataloaders: {time() - start_time:.2f}s')\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_mae(reg_col, dl):\n",
    "    encoder = Encoder(vae_utils.n_letters, NLATENT, DECODER_HIDDEN_SIZE).to(device)\n",
    "    property_predictor = PropertyPredictor(NLATENT).to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(pathlib.Path(MODELS_DIR, f'{reg_col}_encoder.pth')))\n",
    "    property_predictor.load_state_dict(torch.load(pathlib.Path(MODELS_DIR, f'{reg_col}_property_predictor.pth')))\n",
    "    \n",
    "    encoder.eval()\n",
    "    property_predictor.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        abs_errors = []\n",
    "        for input_tensors, target_tensors, original_lengths, property_values in dl:\n",
    "            z, mean, log_var, dec_init_hidden = encoder(input_tensors)\n",
    "            reg_pred = property_predictor(z)\n",
    "            abs_error = torch.abs(property_values - reg_pred.flatten())\n",
    "            abs_errors.append(abs_error)\n",
    "            \n",
    "        abs_errors = torch.cat(abs_errors)\n",
    "        mae_error = abs_errors.mean()\n",
    "        \n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pct = .1\n",
    "bs = 120\n",
    "# max_samples = 1000\n",
    "max_samples = 250000\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "epochs = 120\n",
    "anneal_start = 29\n",
    "lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.61 GiB (GPU 0; 3.95 GiB total capacity; 0 bytes already allocated; 3.02 GiB free; 0 bytes reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9b0e27892a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreg_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'logP'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_valid_test_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a9e7b47bfe4d>\u001b[0m in \u001b[0;36mget_dataloaders\u001b[0;34m(df, train_idx, valid_idx, test_idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-36bbbb53284a>\u001b[0m in \u001b[0;36mget_dl\u001b[0;34m(self, df, idx, bs, device, shuffle)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mtarget_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.61 GiB (GPU 0; 3.95 GiB total capacity; 0 bytes already allocated; 3.02 GiB free; 0 bytes reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "reg_col = 'logP'\n",
    "df, train_idx, valid_idx, test_idx = vae_utils.get_train_valid_test_splits(reg_col, valid_pct)\n",
    "train_dl, valid_dl, test_dl = get_dataloaders(df, train_idx, valid_idx, test_idx)\n",
    "train_losses, val_losses = fit(epochs, anneal_start, lr, train_dl, valid_dl, device, reg_col)\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mean_train = df.iloc[train_idx].reg_col.mean()\n",
    "mean_mae_test = np.mean(np.abs(df.iloc[test_idx].reg_col - reg_mean_train))\n",
    "vae_mae_test = get_pred_mae(reg_col, test_dl).item()\n",
    "errors[reg_col] = {'mean_mae_test': mean_mae_test, 'vae_mae_test': vae_mae_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_col = 'qed'\n",
    "df, train_idx, valid_idx, test_idx = vae_utils.get_train_valid_test_splits(reg_col, valid_pct)\n",
    "train_dl, valid_dl, test_dl = get_dataloaders(df, train_idx, valid_idx, test_idx)\n",
    "train_losses, val_losses = fit(epochs, anneal_start, lr, train_dl, valid_dl, device, reg_col)\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mean_train = df.iloc[train_idx].reg_col.mean()\n",
    "mean_mae_test = np.mean(np.abs(df.iloc[test_idx].reg_col - reg_mean_train))\n",
    "vae_mae_test = get_pred_mae(reg_col, test_dl).item()\n",
    "errors[reg_col] = {'mean_mae_test': mean_mae_test, 'vae_mae_test': vae_mae_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(errors).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
