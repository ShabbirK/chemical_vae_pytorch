{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 120\n",
    "NLATENT = 196\n",
    "DECODER_HIDDEN_SIZE = 488\n",
    "DECODER_NUM_LAYERS = 3\n",
    "\n",
    "DATA_DIR = './data'\n",
    "DATA_FILE_NAME = '250k_rndm_zinc_drugs_clean_3.csv'\n",
    "CHAR_FILE_NAME = 'zinc.json'\n",
    "TEST_IDX_FILE_NAME = 'test_idx.npy'\n",
    "\n",
    "ALL_LETTERS = yaml.safe_load(open(pathlib.Path(DATA_DIR, CHAR_FILE_NAME))) + ['SOS']\n",
    "N_LETTERS = len(ALL_LETTERS) \n",
    "\n",
    "MODELS_DIR = './models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEUtils:\n",
    "    '''\n",
    "    This purpose of this class is to help with various aspects\n",
    "    of data processing\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data_dir=DATA_DIR, \n",
    "                 data_file_name=DATA_FILE_NAME, \n",
    "                 test_idx_file_name=TEST_IDX_FILE_NAME,\n",
    "                 max_len=MAX_LEN,\n",
    "                 all_letters=ALL_LETTERS):\n",
    "        \n",
    "        self.data_dir = pathlib.Path(data_dir)\n",
    "        self.data_file = self.data_dir / pathlib.Path(data_file_name)\n",
    "        self.test_file = self.data_dir / pathlib.Path(test_idx_file_name)\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.all_letters = all_letters\n",
    "        self.n_letters = len(all_letters)\n",
    "        self.letters_to_indices_dict = dict((l, i) for i, l in enumerate(all_letters))\n",
    "        self.indices_to_letters_dict = dict((i, l) for i, l in enumerate(all_letters))\n",
    "               \n",
    "    def get_data_df(self):\n",
    "        df = pd.read_csv(self.data_file)\n",
    "        df = df[df.smiles.str.len() <= self.max_len].reset_index(drop=True)\n",
    "        \n",
    "        # preprocess input smile to remove the newline character and add padding\n",
    "        df.loc[:, 'smiles'] = df.loc[:, 'smiles'].str.strip()\\\n",
    "                    .str.pad(width=self.max_len, side='right', fillchar=\" \")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    # One-hot matrix of first to last letters (not including EOS) for input\n",
    "    def get_input_tensor(self, smile):\n",
    "        tensor = torch.zeros(1, len(smile), self.n_letters) # batch_size * seq_length * num_features\n",
    "        for i, letter in enumerate(smile):\n",
    "            tensor[0][i][self.letters_to_indices_dict[letter]] = 1\n",
    "        return tensor\n",
    "\n",
    "    # LongTensor of first letter to end (EOS) for target\n",
    "    def get_target_tensor(self, smile):\n",
    "        letter_indexes = [self.letters_to_indices_dict[l] for l in smile]\n",
    "        # letter_indexes.append(self.n_letters - 1) # EOS\n",
    "        return torch.LongTensor(letter_indexes)\n",
    "    \n",
    "    def get_train_valid_test_splits(self, reg_col, valid_pct=.1):\n",
    "        df = self.get_data_df()[['smiles', reg_col]]\n",
    "        df = df.rename(columns={reg_col: 'reg_col'})\n",
    "        \n",
    "        test_idx = np.load(self.test_file)\n",
    "        non_test_idx = np.array(df[~df.index.isin(test_idx)].index)\n",
    "        train_idx, valid_idx = train_test_split(non_test_idx, test_size=valid_pct, \n",
    "                                                random_state=42, shuffle=True)\n",
    "        \n",
    "        assert len(df) == len(test_idx) + len(train_idx) + len(valid_idx)\n",
    "        \n",
    "        return df, train_idx, valid_idx, test_idx\n",
    "         \n",
    "    def get_dl(self, df, idx, bs, shuffle=False):\n",
    "        \n",
    "        df = df.iloc[idx]\n",
    "        \n",
    "        input_tensors = torch.zeros(len(df), self.max_len, self.n_letters)\n",
    "        target_tensors = torch.zeros(len(df), self.max_len, dtype=torch.long)\n",
    "        for i, smile in enumerate(tqdm(df.smiles)):\n",
    "            input_tensors[i] = self.get_input_tensor(smile)\n",
    "            target_tensors[i] = self.get_target_tensor(smile)\n",
    "        \n",
    "        input_tensors = input_tensors\n",
    "        target_tensors = target_tensors\n",
    "        \n",
    "        # original_lengths = torch.tensor(df.smiles.str.strip().str.len().to_numpy())\n",
    "\n",
    "        property_values = torch.tensor(df.reg_col.to_numpy()).type(torch.float32)\n",
    "         \n",
    "        ds = TensorDataset(input_tensors, target_tensors, property_values)\n",
    "        dl = DataLoader(ds, shuffle=shuffle, batch_size=bs)\n",
    "        \n",
    "        return dl\n",
    "    \n",
    "vae_utils = VAEUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    '''\n",
    "    This class simplifies layers from \n",
    "    custom functions\n",
    "    '''\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_letters, nlatent, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.n_letters = n_letters\n",
    "        self.nlatent = nlatent\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            Lambda(lambda x: x.permute(0, 2, 1)), # the features are in the channels dimension\n",
    "            nn.Conv1d(in_channels=n_letters, out_channels=9, kernel_size=9),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(9), # the authors of the paper did batch normalization\n",
    "            nn.Conv1d(in_channels=9, out_channels=9, kernel_size=9),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(9), # the authors of the paper did batch normalization\n",
    "            nn.Conv1d(in_channels=9, out_channels=11, kernel_size=10),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(11), # the authors of the paper did batch normalization\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.mean = nn.Linear(1045, nlatent)\n",
    "        self.log_var = nn.Linear(1045, nlatent)\n",
    "        self.dec_init_hidden = nn.Linear(nlatent, decoder_hidden_size)\n",
    "        \n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var) \n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mean + (eps * std) \n",
    "        return sample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mean(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        dec_init_hidden = self.dec_init_hidden(z)\n",
    "        \n",
    "        return z, mean, log_var, dec_init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_decoder_input(input_tensors):\n",
    "    '''\n",
    "    Adjust for SOS and make batch the second demension\n",
    "    '''\n",
    "    sos_tensor = torch.zeros(1, N_LETTERS)\n",
    "    sos_tensor[0][vae_utils.letters_to_indices_dict['SOS']] = 1\n",
    "    new_tensor = torch.zeros(input_tensors.shape[0], MAX_LEN, N_LETTERS)\n",
    "    new_tensor[:][0] = sos_tensor\n",
    "    new_tensor[:, 1:MAX_LEN, :] = input_tensors[:, 0:MAX_LEN-1, :]\n",
    "    new_tensor = new_tensor.permute(1, 0, 2).to(input_tensors.device)\n",
    "    \n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.preprocess = Lambda(preprocess_decoder_input)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.rnn(self.preprocess(input), )\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyPredictor(nn.Module):\n",
    "    def __init__(self, nlatent):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(nlatent, 1000),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(1000, 1000),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(1000, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.predictor(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Helper Classes & Functions for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, reg_col):\n",
    "        \n",
    "        models_dir = pathlib.Path(MODELS_DIR)\n",
    "        models_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.encoder_file = models_dir / pathlib.Path(f'{reg_col}_encoder.pth')\n",
    "        self.decoder_file = models_dir / pathlib.Path(f'{reg_col}_decoder.pth')\n",
    "        self.property_predictor_file = models_dir / pathlib.Path(f'{reg_col}_property_predictor.pth')\n",
    "        self.train_losses_file = models_dir / pathlib.Path(f'{reg_col}_train_losses.csv')\n",
    "        self.val_losses_file = models_dir / pathlib.Path(f'{reg_col}_valid_losses.csv')\n",
    "\n",
    "        self.loss_columns = ['total_loss', 'reconstruction_loss', 'kl_divergence', 'regression_loss']\n",
    "        self.train_losses_df = pd.DataFrame(columns=self.loss_columns)\n",
    "        self.val_losses_df = pd.DataFrame(columns=self.loss_columns)\n",
    "\n",
    "    def intitialize_networks(self):\n",
    "        self.encoder = Encoder(vae_utils.n_letters, NLATENT, DECODER_HIDDEN_SIZE)\n",
    "        self.decoder = Decoder(vae_utils.n_letters, DECODER_HIDDEN_SIZE, DECODER_NUM_LAYERS)\n",
    "        self.property_predictor = PropertyPredictor(NLATENT)\n",
    "        \n",
    "    def __time_since(self, since):\n",
    "        now = time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return f'{m}m {s:.0f}s'     \n",
    "        \n",
    "    def kl_anneal_function(self, epoch, anneal_start, k=1):\n",
    "        return 1 / (1 + np.exp(- k * (epoch - anneal_start)))\n",
    "    \n",
    "    def get_losses(self, epoch, input_tensors, target_tensors, property_values):\n",
    "                \n",
    "        recontruction_loss_func = nn.NLLLoss()\n",
    "        reg_loss_func = nn.MSELoss()\n",
    "        \n",
    "        input_tensors = input_tensors.to(device)\n",
    "        target_tensors = target_tensors.to(device)\n",
    "        property_values = property_values.to(device)\n",
    "\n",
    "        z, mean, log_var, dec_init_hidden = self.encoder(input_tensors)\n",
    "        output, hidden = self.decoder(input_tensors, \n",
    "                                 dec_init_hidden.unsqueeze(0).repeat(DECODER_NUM_LAYERS, 1, 1))\n",
    "        reg_pred = self.property_predictor(z)\n",
    "\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        kl_weight = self.kl_anneal_function(epoch, anneal_start)\n",
    "\n",
    "        reg_loss = reg_loss_func(reg_pred.flatten(), property_values)\n",
    "        reg_loss = reg_loss.type(torch.float32) \n",
    "\n",
    "        reconstruction_loss = 0\n",
    "        for i in range(input_tensors.shape[0]):\n",
    "            reconstruction_loss += recontruction_loss_func(output[i], target_tensors[i])\n",
    "        reconstruction_loss /= input_tensors.shape[0]\n",
    "\n",
    "        loss = reconstruction_loss + kl_divergence * kl_weight + reg_loss\n",
    "        \n",
    "        return loss, reconstruction_loss, kl_divergence, reg_loss\n",
    "    \n",
    "    def process_dl(self, epoch, anneal_start, device, dl, train, \n",
    "                   enc_opt=None, dec_opt=None, pp_opt=None):\n",
    "        \n",
    "        num_loaders = len(dl)\n",
    "        loader_loss = 0\n",
    "        recon_loss_epoch = 0\n",
    "        kld_epoch = 0\n",
    "        reg_loss_epoch = 0\n",
    "        \n",
    "        for input_tensors, target_tensors, property_values in dl:\n",
    "            loss, reconstruction_loss, kl_divergence, reg_loss = \\\n",
    "                self.get_losses(epoch, input_tensors, target_tensors, property_values)\n",
    "            \n",
    "            if train:\n",
    "                enc_opt.zero_grad()\n",
    "                dec_opt.zero_grad()\n",
    "                pp_opt.zero_grad()\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                enc_opt.step()\n",
    "                dec_opt.step()\n",
    "                pp_opt.step()\n",
    "                \n",
    "            loader_loss += loss.item()\n",
    "            recon_loss_epoch += reconstruction_loss.item()\n",
    "            kld_epoch += kl_divergence.item()\n",
    "            reg_loss_epoch += reg_loss.item()\n",
    "        \n",
    "        loader_loss /= num_loaders\n",
    "        recon_loss_epoch /= num_loaders\n",
    "        kld_epoch /= num_loaders\n",
    "        reg_loss_epoch /= num_loaders\n",
    "        \n",
    "        return (loader_loss, recon_loss_epoch, kld_epoch, reg_loss_epoch)\n",
    "    \n",
    "    def save_parameters_and_losses(self, train_losses:list, val_losses:list):\n",
    "        \n",
    "        torch.save(self.encoder.state_dict(), self.encoder_file)\n",
    "        torch.save(self.decoder.state_dict(), self.decoder_file)\n",
    "        torch.save(self.property_predictor.state_dict(), self.property_predictor_file)\n",
    "        \n",
    "        temp_df = pd.DataFrame(data=train_losses, columns=self.loss_columns)\n",
    "        self.train_losses_df = self.train_losses_df.append(temp_df, ignore_index=True)\n",
    "        self.train_losses_df.to_csv(self.train_losses_file, index=False)\n",
    "        \n",
    "        temp_df = pd.DataFrame(data=val_losses, columns=self.loss_columns)\n",
    "        self.val_losses_df = self.val_losses_df.append(temp_df, ignore_index=True)\n",
    "        self.val_losses_df.to_csv(self.val_losses_file, index=False)\n",
    "        \n",
    "    def load_parameters_and_losses(self):\n",
    "        self.encoder.load_state_dict(torch.load(self.encoder_file))\n",
    "        self.decoder.load_state_dict(torch.load(self.decoder_file))\n",
    "        self.property_predictor.load_state_dict(torch.load(self.property_predictor_file))\n",
    "        self.train_losses_df = pd.read_csv(self.train_losses_file)\n",
    "        self.val_losses_df = pd.read_csv(self.val_losses_file)\n",
    "        \n",
    "    \n",
    "    def fit(self, epochs, save_every, anneal_start, lr, train_dl, valid_dl, device, load_previous=False):\n",
    "        \n",
    "        self.intitialize_networks()\n",
    "    \n",
    "        if load_previous:\n",
    "            self.load_parameters_and_losses()\n",
    "            \n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.decoder = self.decoder.to(device)\n",
    "        self.property_predictor = self.property_predictor.to(device)\n",
    "\n",
    "        enc_opt = optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        dec_opt = optim.Adam(self.decoder.parameters(), lr=lr)\n",
    "        pp_opt = optim.Adam(self.property_predictor.parameters(), lr=lr)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        prev_n_epochs = len(self.train_losses_df) \n",
    "\n",
    "        start_time = time()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            self.property_predictor.train()\n",
    "            train_loss = self.process_dl(prev_n_epochs + epoch, anneal_start, device, \n",
    "                                           train_dl, True, enc_opt, dec_opt, pp_opt)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            self.encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            self.property_predictor.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.process_dl(prev_n_epochs + epoch, anneal_start, device,\n",
    "                                             valid_dl, False)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "            # if epoch % 5 == 4:\n",
    "            print(f\"Epoch: {prev_n_epochs + epoch + 1:3d} | \" + \\\n",
    "                  f\"Train Loss: {train_loss[0]:10.5f} | Val Loss: {val_loss[0]:10.5f} | \" + \\\n",
    "                  f\"Time Taken: {self.__time_since(start_time)}\")\n",
    "            print(f\"Train Recon Loss: {train_loss[1]:10.5f} | Val Recon Loss: {val_loss[1]:10.5f}\")\n",
    "            print(f\"Train KLD: {train_loss[2]:10.5f} | Val KLD: {val_loss[2]:10.5f}\")\n",
    "            print(f\"Train Reg Loss: {train_loss[3]:10.5f} | Val Reg Loss: {val_loss[3]:10.5f}\\n\")\n",
    "            \n",
    "            if epoch % save_every == save_every - 1:\n",
    "                self.save_parameters_and_losses(train_losses, val_losses)\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "            \n",
    "        self.save_parameters_and_losses(train_losses, val_losses)\n",
    "        \n",
    "        del self.encoder\n",
    "        del self.decoder\n",
    "        del self.property_predictor\n",
    "        \n",
    "    def plot_losses(self):\n",
    "        \n",
    "        train_losses_df = pd.read_csv(self.train_losses_file)\n",
    "        val_losses_df = pd.read_csv(self.val_losses_file)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))\n",
    "        \n",
    "                \n",
    "        ax1.plot(train_losses_df['total_loss'], label='Train')\n",
    "        ax1.plot(val_losses_df['total_loss'], label='Validation')\n",
    "        ax1.set_ylabel('Total Loss')\n",
    "        ax1.legend(loc='upper right')\n",
    "        \n",
    "        ax2.plot(train_losses_df['reconstruction_loss'], label='Train')\n",
    "        ax2.plot(val_losses_df['reconstruction_loss'], label='Validation')\n",
    "        ax2.set_ylabel('Reconstruction Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        \n",
    "        ax3.plot(train_losses_df['kl_divergence'], label='Train')\n",
    "        ax3.plot(val_losses_df['kl_divergence'], label='Validation')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('KL Divergence')\n",
    "        ax3.legend(loc='upper right')\n",
    "        \n",
    "        \n",
    "        ax4.plot(train_losses_df['regression_loss'], label='Train')\n",
    "        ax4.plot(val_losses_df['regression_loss'], label='Validation')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Regression Loss')\n",
    "        ax4.legend(loc='upper right')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df, train_idx, valid_idx, max_samples, bs):\n",
    "    start_time = time()\n",
    "    train_dl = vae_utils.get_dl(df, train_idx[:max_samples], bs, shuffle=True)\n",
    "    valid_dl = vae_utils.get_dl(df, valid_idx[:max_samples], bs)\n",
    "    print(f'Time taken to get dataloaders: {time() - start_time:.2f}s')\n",
    "    \n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_mae(reg_col, df, test_idx, bs):\n",
    "    encoder = Encoder(vae_utils.n_letters, NLATENT, DECODER_HIDDEN_SIZE)\n",
    "    property_predictor = PropertyPredictor(NLATENT)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(pathlib.Path(MODELS_DIR, f'{reg_col}_encoder.pth')))\n",
    "    property_predictor.load_state_dict(torch.load(pathlib.Path(MODELS_DIR, f'{reg_col}_property_predictor.pth')))\n",
    "    \n",
    "    dl = vae_utils.get_dl(df, test_idx, bs)\n",
    "    \n",
    "    encoder.eval()\n",
    "    property_predictor.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        abs_errors = []\n",
    "        for input_tensors, target_tensors, property_values in dl:\n",
    "            z, mean, log_var, dec_init_hidden = encoder(input_tensors)\n",
    "            reg_pred = property_predictor(z)\n",
    "            abs_error = torch.abs(property_values - reg_pred.flatten())\n",
    "            abs_errors.append(abs_error)\n",
    "            \n",
    "        abs_errors = torch.cat(abs_errors)\n",
    "        mae_error = abs_errors.mean()\n",
    "        \n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pct = .1\n",
    "bs = 2048\n",
    "max_samples = 250000\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "epochs = 120\n",
    "save_every = 2\n",
    "anneal_start = 29\n",
    "lr = 0.0005\n",
    "load_previous = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Network for _logP_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224018/224018 [04:20<00:00, 859.99it/s]\n",
      "100%|██████████| 24891/24891 [00:28<00:00, 862.10it/s]\n",
      "100%|██████████| 546/546 [00:00<00:00, 911.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to get dataloaders: 290.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [04:30<8:55:52, 270.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | Train Loss:    2.42376 | Val Loss:    1.38142 | Time Taken: 4m 30s\n",
      "Train Recon Loss:    1.18179 | Val Recon Loss:    0.89172\n",
      "Train KLD: 263673.48295 | Val KLD: 397913.70433\n",
      "Train Reg Loss:    1.24197 | Val Reg Loss:    0.48969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/120 [08:59<8:50:26, 269.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   2 | Train Loss:    1.00560 | Val Loss:    0.74065 | Time Taken: 8m 59s\n",
      "Train Recon Loss:    0.69357 | Val Recon Loss:    0.55042\n",
      "Train KLD: 477994.25312 | Val KLD: 501746.81130\n",
      "Train Reg Loss:    0.31203 | Val Reg Loss:    0.19023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 3/120 [13:30<8:46:44, 270.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   3 | Train Loss:    0.65418 | Val Loss:    0.55034 | Time Taken: 13m 30s\n",
      "Train Recon Loss:    0.48239 | Val Recon Loss:    0.42963\n",
      "Train KLD: 555387.07116 | Val KLD: 552097.56550\n",
      "Train Reg Loss:    0.17180 | Val Reg Loss:    0.12071\n",
      "\n",
      "Epoch:   4 | Train Loss:    0.52617 | Val Loss:    0.47782 | Time Taken: 18m 0s\n",
      "Train Recon Loss:    0.40147 | Val Recon Loss:    0.37639\n",
      "Train KLD: 621789.10795 | Val KLD: 622369.07993\n",
      "Train Reg Loss:    0.12469 | Val Reg Loss:    0.10143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/120 [22:31<8:38:23, 270.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   5 | Train Loss:    0.47541 | Val Loss:    0.48934 | Time Taken: 22m 31s\n",
      "Train Recon Loss:    0.36258 | Val Recon Loss:    0.34613\n",
      "Train KLD: 666524.28906 | Val KLD: 626949.27524\n",
      "Train Reg Loss:    0.11282 | Val Reg Loss:    0.14321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/120 [27:01<8:33:28, 270.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   6 | Train Loss:    0.43188 | Val Loss:    0.41901 | Time Taken: 27m 1s\n",
      "Train Recon Loss:    0.33609 | Val Recon Loss:    0.34752\n",
      "Train KLD: 698316.64830 | Val KLD: 671604.25481\n",
      "Train Reg Loss:    0.09576 | Val Reg Loss:    0.07147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7/120 [31:32<8:29:19, 270.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   7 | Train Loss:    0.40787 | Val Loss:    0.38696 | Time Taken: 31m 32s\n",
      "Train Recon Loss:    0.31935 | Val Recon Loss:    0.30938\n",
      "Train KLD: 727872.33494 | Val KLD: 699678.96394\n",
      "Train Reg Loss:    0.08844 | Val Reg Loss:    0.07751\n",
      "\n",
      "Epoch:   8 | Train Loss:    0.38395 | Val Loss:    0.43490 | Time Taken: 36m 2s\n",
      "Train Recon Loss:    0.30482 | Val Recon Loss:    0.29961\n",
      "Train KLD: 760455.57841 | Val KLD: 717516.14663\n",
      "Train Reg Loss:    0.07892 | Val Reg Loss:    0.13509\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 9/120 [40:34<8:21:01, 270.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   9 | Train Loss:    0.37823 | Val Loss:    0.35110 | Time Taken: 40m 34s\n",
      "Train Recon Loss:    0.29392 | Val Recon Loss:    0.28823\n",
      "Train KLD: 761147.79034 | Val KLD: 696151.78245\n",
      "Train Reg Loss:    0.08373 | Val Reg Loss:    0.06234\n",
      "\n",
      "Epoch:  10 | Train Loss:    0.35688 | Val Loss:    0.33741 | Time Taken: 45m 5s\n",
      "Train Recon Loss:    0.28476 | Val Recon Loss:    0.28063\n",
      "Train KLD: 754408.13580 | Val KLD: 713673.07332\n",
      "Train Reg Loss:    0.07057 | Val Reg Loss:    0.05532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/120 [49:36<8:12:14, 270.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11 | Train Loss:    0.35428 | Val Loss:    0.33398 | Time Taken: 49m 36s\n",
      "Train Recon Loss:    0.27996 | Val Recon Loss:    0.27508\n",
      "Train KLD: 714360.29830 | Val KLD: 625623.31611\n",
      "Train Reg Loss:    0.07032 | Val Reg Loss:    0.05540\n",
      "\n",
      "Epoch:  12 | Train Loss:    0.35010 | Val Loss:    0.32576 | Time Taken: 54m 6s\n",
      "Train Recon Loss:    0.27265 | Val Recon Loss:    0.26949\n",
      "Train KLD: 560804.62670 | Val KLD: 464944.85457\n",
      "Train Reg Loss:    0.06890 | Val Reg Loss:    0.04919\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 13/120 [58:38<8:03:17, 271.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13 | Train Loss:    0.35281 | Val Loss:    0.34637 | Time Taken: 58m 38s\n",
      "Train Recon Loss:    0.26731 | Val Recon Loss:    0.26553\n",
      "Train KLD: 353644.58629 | Val KLD: 291051.82843\n",
      "Train Reg Loss:    0.07086 | Val Reg Loss:    0.06879\n",
      "\n",
      "Epoch:  14 | Train Loss:    0.36521 | Val Loss:    0.39037 | Time Taken: 63m 9s\n",
      "Train Recon Loss:    0.26605 | Val Recon Loss:    0.26173\n",
      "Train KLD: 214898.17173 | Val KLD: 178387.08428\n",
      "Train Reg Loss:    0.07498 | Val Reg Loss:    0.10856\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 15/120 [1:07:40<7:54:27, 271.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15 | Train Loss:    0.38132 | Val Loss:    0.35600 | Time Taken: 67m 40s\n",
      "Train Recon Loss:    0.26018 | Val Recon Loss:    0.25995\n",
      "Train KLD: 130615.86321 | Val KLD: 106427.67803\n",
      "Train Reg Loss:    0.08118 | Val Reg Loss:    0.06350\n",
      "\n",
      "Epoch:  16 | Train Loss:    0.40291 | Val Loss:    0.39077 | Time Taken: 72m 10s\n",
      "Train Recon Loss:    0.25664 | Val Recon Loss:    0.25484\n",
      "Train KLD: 72825.77056 | Val KLD: 57597.44404\n",
      "Train Reg Loss:    0.08572 | Val Reg Loss:    0.08803\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 17/120 [1:16:40<7:44:43, 270.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17 | Train Loss:    0.43758 | Val Loss:    0.39000 | Time Taken: 76m 41s\n",
      "Train Recon Loss:    0.25428 | Val Recon Loss:    0.25293\n",
      "Train KLD: 37024.06325 | Val KLD: 30136.00053\n",
      "Train Reg Loss:    0.09961 | Val Reg Loss:    0.06895\n",
      "\n",
      "Epoch:  18 | Train Loss:    0.49727 | Val Loss:    0.57935 | Time Taken: 81m 11s\n",
      "Train Recon Loss:    0.25746 | Val Recon Loss:    0.25086\n",
      "Train KLD: 19808.79817 | Val KLD: 18861.79002\n",
      "Train Reg Loss:    0.11810 | Val Reg Loss:    0.21260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 19/120 [1:25:45<7:37:13, 271.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19 | Train Loss:    0.58217 | Val Loss:    0.51458 | Time Taken: 85m 45s\n",
      "Train Recon Loss:    0.24857 | Val Recon Loss:    0.24845\n",
      "Train KLD: 11205.97558 | Val KLD: 10033.31150\n",
      "Train Reg Loss:    0.14645 | Val Reg Loss:    0.09856\n",
      "\n",
      "Epoch:  20 | Train Loss:    0.74688 | Val Loss:    0.68686 | Time Taken: 90m 16s\n",
      "Train Recon Loss:    0.24603 | Val Recon Loss:    0.24633\n",
      "Train KLD: 6410.26089 | Val KLD: 6211.34798\n",
      "Train Reg Loss:    0.20983 | Val Reg Loss:    0.15854\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 21/120 [1:34:49<7:29:00, 272.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21 | Train Loss:    0.96316 | Val Loss:    0.81306 | Time Taken: 94m 50s\n",
      "Train Recon Loss:    0.24435 | Val Recon Loss:    0.24477\n",
      "Train KLD: 3772.97719 | Val KLD: 3253.62854\n",
      "Train Reg Loss:    0.25324 | Val Reg Loss:    0.16681\n",
      "\n",
      "Epoch:  22 | Train Loss:    1.67968 | Val Loss:    1.29435 | Time Taken: 99m 21s\n",
      "Train Recon Loss:    0.24222 | Val Recon Loss:    0.24237\n",
      "Train KLD: 2821.88906 | Val KLD: 1987.66743\n",
      "Train Reg Loss:    0.49114 | Val Reg Loss:    0.38541\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 23/120 [1:43:52<7:19:04, 271.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23 | Train Loss:    2.29750 | Val Loss:    2.02929 | Time Taken: 103m 52s\n",
      "Train Recon Loss:    0.24066 | Val Recon Loss:    0.24026\n",
      "Train KLD: 1085.85870 | Val KLD:  898.92842\n",
      "Train Reg Loss:    1.06757 | Val Reg Loss:    0.97006\n",
      "\n",
      "Epoch:  24 | Train Loss:    4.13956 | Val Loss:    2.44916 | Time Taken: 108m 22s\n",
      "Train Recon Loss:    0.24237 | Val Recon Loss:    0.25369\n",
      "Train KLD:  751.62386 | Val KLD:   47.88138\n",
      "Train Reg Loss:    2.03871 | Val Reg Loss:    2.07707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 25/120 [1:52:53<7:09:13, 271.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  25 | Train Loss:    2.51757 | Val Loss:    2.49287 | Time Taken: 112m 53s\n",
      "Train Recon Loss:    0.23832 | Val Recon Loss:    0.23721\n",
      "Train KLD:   28.81396 | Val KLD:   26.60832\n",
      "Train Reg Loss:    2.08640 | Val Reg Loss:    2.07757\n",
      "\n",
      "Epoch:  26 | Train Loss:    5.71003 | Val Loss:    4.33630 | Time Taken: 117m 23s\n",
      "Train Recon Loss:    0.23529 | Val Recon Loss:    0.23605\n",
      "Train KLD:  188.13993 | Val KLD:  111.70041\n",
      "Train Reg Loss:    2.09081 | Val Reg Loss:    2.09119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 27/120 [2:01:54<6:59:42, 270.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  27 | Train Loss:    7.38696 | Val Loss:   12.92414 | Time Taken: 121m 54s\n",
      "Train Recon Loss:    0.23385 | Val Recon Loss:    0.23424\n",
      "Train KLD:  106.78203 | Val KLD:  223.76273\n",
      "Train Reg Loss:    2.08889 | Val Reg Loss:    2.07775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_col = 'logP'\n",
    "df, train_idx, valid_idx, test_idx = vae_utils.get_train_valid_test_splits(reg_col, valid_pct)\n",
    "train_dl, valid_dl = get_dataloaders(df, train_idx, valid_idx, max_samples, bs)\n",
    "trainer = Trainer(reg_col)\n",
    "trainer.fit(epochs, save_every, anneal_start, lr, train_dl, valid_dl, device, load_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mean_train = df.iloc[train_idx].reg_col.mean()\n",
    "mean_mae_test = np.mean(np.abs(df.iloc[test_idx].reg_col - reg_mean_train))\n",
    "vae_mae_test = get_pred_mae(reg_col, df, test_idx, bs).item()\n",
    "errors[reg_col] = {'mean_mae_test': mean_mae_test, 'vae_mae_test': vae_mae_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Network for _QED_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_col = 'qed'\n",
    "df, train_idx, valid_idx, test_idx = vae_utils.get_train_valid_test_splits(reg_col, valid_pct)\n",
    "train_dl, valid_dl = get_dataloaders(df, train_idx, valid_idx, max_samples, bs)\n",
    "trainer = Trainer(reg_col)\n",
    "trainer.fit(epochs, save_every, anneal_start, lr, train_dl, valid_dl, device, load_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mean_train = df.iloc[train_idx].reg_col.mean()\n",
    "mean_mae_test = np.mean(np.abs(df.iloc[test_idx].reg_col - reg_mean_train))\n",
    "vae_mae_test = get_pred_mae(reg_col, df, test_idx, bs).item()\n",
    "errors[reg_col] = {'mean_mae_test': mean_mae_test, 'vae_mae_test': vae_mae_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(errors).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was trained in 4 stages. Except for the 1st stage, the other two stages were trained after loading the saving parameters from the previous stage.\n",
    "The stages are summarized below:\n",
    "\n",
    "1. max_exmaples = 20,000, epochs = 50, anneal_start = 15, load_previous = False, bs = 200\n",
    "2. max_examples = 100,000, epochs = 15, anneal_start = 5, load_previous = True, bs = 200\n",
    "3. max_examples = all, epochs = 10, anneal_start = 3, load_previous = True, bs = 200\n",
    "4. max_examples = all, epochs = 15, anneal_start = 5, load_previous = True, bs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
